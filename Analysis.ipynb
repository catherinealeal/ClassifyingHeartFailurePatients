{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 560,
   "id": "6a1c81dd-62c1-4e79-b686-635b38d61c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06649fc4-00fc-4276-9702-ecafaa747580",
   "metadata": {},
   "source": [
    "# Comparing Classifiers for Predicting Death in Heart Failure Patients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d6b5b1-ef3c-4764-94ee-84d9f162dfb7",
   "metadata": {},
   "source": [
    "## Introduction and Goal\n",
    "\n",
    "Heart failure is a condition where the heart becomes too weak to pump blood effectively throughout the body. Patient outcomes can vary widely and often depend on many biological factors.\n",
    "\n",
    "**The goal of this project is to train and compare three classifiers for predicting survival in heart failure patients.** The models I’ll be using are K-Nearest Neighbors, Gaussian Naive Bayes, and Logistic Regression. Each model will learn from biological features to predict whether a patient survived or not.\n",
    "\n",
    "The process will be as follows: first, train all three models and evaluate them using multiple performance metrics. Next, refine the models through parameter tuning and dimensionality reduction. Finally, compare their performances and determine which model provides the most reliable predictions.\n",
    "\n",
    "## Data Description\n",
    "\n",
    "The dataset contains information on 299 patients with heart failure. Each row represents one patient, and each of the 13 columns provides a biological or clinical feature:\n",
    "\n",
    "- Age\n",
    "- Sex\n",
    "- Whether they are anaemic\n",
    "- Whether they have hypertension\n",
    "- CPK enzyme level\n",
    "- Whether they have diabetes\n",
    "- Platelet count\n",
    "- Serum creatinine level\n",
    "- Serum sodium level\n",
    "- Smoking status\n",
    "- Follow-up period\n",
    "- Death event\n",
    "\n",
    "The follow-up period indicates how long a patient was monitored after their heart failure diagnosis, with an average of 130 days. The death event column is the target variable and records whether the patient survived (0) or died (1) during follow-up.\n",
    "\n",
    "For prediction, I’ll use the first 11 features. Since all of them are numeric or already encoded numerically, the dataset is ready for training without additional preprocessing.\n",
    "\n",
    "Access the data [here](https://archive.ics.uci.edu/dataset/519/heart+failure+clinical+records)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1038eb-232f-4c37-a53e-617fdd2fa08d",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "id": "297b3c79-425e-4d21-8ca0-4c856292d935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>anaemia</th>\n",
       "      <th>creatinine_phosphokinase</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>ejection_fraction</th>\n",
       "      <th>high_blood_pressure</th>\n",
       "      <th>platelets</th>\n",
       "      <th>serum_creatinine</th>\n",
       "      <th>serum_sodium</th>\n",
       "      <th>sex</th>\n",
       "      <th>smoking</th>\n",
       "      <th>time</th>\n",
       "      <th>DEATH_EVENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75.0</td>\n",
       "      <td>0</td>\n",
       "      <td>582</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>265000.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>130</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>55.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7861</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>263358.03</td>\n",
       "      <td>1.1</td>\n",
       "      <td>136</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>65.0</td>\n",
       "      <td>0</td>\n",
       "      <td>146</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>162000.00</td>\n",
       "      <td>1.3</td>\n",
       "      <td>129</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1</td>\n",
       "      <td>111</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>210000.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>137</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>65.0</td>\n",
       "      <td>1</td>\n",
       "      <td>160</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>327000.00</td>\n",
       "      <td>2.7</td>\n",
       "      <td>116</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age  anaemia  creatinine_phosphokinase  diabetes  ejection_fraction  \\\n",
       "0  75.0        0                       582         0                 20   \n",
       "1  55.0        0                      7861         0                 38   \n",
       "2  65.0        0                       146         0                 20   \n",
       "3  50.0        1                       111         0                 20   \n",
       "4  65.0        1                       160         1                 20   \n",
       "\n",
       "   high_blood_pressure  platelets  serum_creatinine  serum_sodium  sex  \\\n",
       "0                    1  265000.00               1.9           130    1   \n",
       "1                    0  263358.03               1.1           136    1   \n",
       "2                    0  162000.00               1.3           129    1   \n",
       "3                    0  210000.00               1.9           137    1   \n",
       "4                    0  327000.00               2.7           116    0   \n",
       "\n",
       "   smoking  time  DEATH_EVENT  \n",
       "0        0     4            1  \n",
       "1        0     6            1  \n",
       "2        1     7            1  \n",
       "3        0     7            1  \n",
       "4        0     8            1  "
      ]
     },
     "execution_count": 561,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f53e34-f246-4dc2-b797-a3198eeaf959",
   "metadata": {},
   "source": [
    "Check for missing values: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "id": "e8562914-1dca-4a49-ac7c-a894f2e46788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                         0\n",
       "anaemia                     0\n",
       "creatinine_phosphokinase    0\n",
       "diabetes                    0\n",
       "ejection_fraction           0\n",
       "high_blood_pressure         0\n",
       "platelets                   0\n",
       "serum_creatinine            0\n",
       "serum_sodium                0\n",
       "sex                         0\n",
       "smoking                     0\n",
       "time                        0\n",
       "DEATH_EVENT                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 562,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dda8da-69ee-45bd-9119-ff9b1dedbd40",
   "metadata": {},
   "source": [
    "Check for balance of groups: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "id": "8cd92d64-3ebd-497a-bbdb-0d7cb6821296",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    203\n",
       "1     96\n",
       "Name: DEATH_EVENT, dtype: int64"
      ]
     },
     "execution_count": 563,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['DEATH_EVENT'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d991d1-c35a-4740-9ea5-b070de61d09e",
   "metadata": {},
   "source": [
    "Separate features from target variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "id": "ab4cd6ad-343f-4449-aa70-52c8eb5084bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['DEATH_EVENT', 'time'], axis = 1) # features \n",
    "y = df['DEATH_EVENT'] # target variable \n",
    "y_labels = ['Died' if yi == 1 else 'Survived' for yi in y] # target labels \n",
    "model_names = ['KNN', 'Gaussian NB', 'LR']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717aab1c-1b2b-4c6d-b55b-d509aa309282",
   "metadata": {},
   "source": [
    "Separate into training and testing sets for evaluation purposes. Stratify by the target variable to ensure an equal distribution of individuals in each group. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "id": "cc0962bc-4276-400d-a17c-1d0ffab1d5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    stratify=y, \n",
    "                                                    random_state=26)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecce1b86-788f-41e0-910c-0cdde13e8704",
   "metadata": {},
   "source": [
    "Standardize each feature to have a mean of 0 and variance of 1. This is required for the KNN algorithm so that no features dominate the distance computations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "id": "af15b359-9ad0-4208-8bcb-0d240341e29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180d802d-2e28-4c47-91ce-6851839e21c1",
   "metadata": {},
   "source": [
    "## Analysis: Baseline Models \n",
    "\n",
    "First, I'm going to train and test the 3 models using the 11 training features directly and with default hyper-parameters. \n",
    "\n",
    "**K-Nearest Neighbors (KNN)** is an instance-based, non-parametric algorithm that predicts outcomes based on similarity to nearby data points. Using k = 5 neighbors as the baseline parameter, it classifies a patient by majority vote among the 5 closest points. Unlike probabilistic models, it doesn’t build an explicit model of the data distribution and instead makes predictions directly from the training set. It’s sensitive to feature scaling and can be slower at larger scales, but it’s useful because it makes very few assumptions about the data.\n",
    "\n",
    "**Gaussian Naive Bayes** is a fast, simple probabilistic classifier that applies Bayes’ theorem under the assumption of feature independence. The Gaussian version models continuous features as normally distributed. The performance of this model will depend on how well the normality assumption holds for the training features.\n",
    "\n",
    "**Logistic Regression** is a linear probabilistic model that predicts survival probabilities using a logistic function of the input features. With its default regularization parameter of 1.0, it balances fit and simplicity. Since it doesn’t rely on independence or normality assumptions, it may outperform Naive Bayes when features are correlated or deviate from a Gaussian distribution.\n",
    "\n",
    "Model performance will be evaluated using 4 metrics:\n",
    "\n",
    "- Accuracy: The overall proportion of correct predictions out of all predictions.\n",
    "- Precision: Of the patients predicted to die, the proportion that actually died.\n",
    "- Recall: Of the patients who actually died, the proportion that the model correctly identified.\n",
    "- F1-Score: The harmonic mean of precision and recall.\n",
    "\n",
    "Since the groups are unbalanced (<33% of participants died), it’s important to evaluate model performance using more than just accuracy. A model that predicts all heart failure patients will survive would have an accuracy >67%, despite being essentially useless. The F1-score is a better measure of model fit because it balances the effects of false negatives and false positives. A false negative in this context is a patient who dies but is predicted to survive, while a false positive is the opposite: a patient who survives but is predicted to die. As in most medical cases, it’s better to catch as many at-risk patients as possible, even if that means some patients are flagged who ultimately survive. That said, maintaining a balance is still important, which is why the F1-score is a valuable metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "id": "d7f7bfd8-80b5-4af4-a0c6-1ab4f77f42b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_performances = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "id": "e014687c-8e2b-4489-aa81-83f90adcf348",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainAndTest(model, X_train, y_train, X_test, y_test, name):\n",
    "    \"\"\"\n",
    "    Function to be used for each model's training and testing \n",
    "    - Trains a model \n",
    "    - Make predictions \n",
    "    - Compute performance metrics: accuracy, precision, recall, F1\n",
    "    - Returns a dictionary of performance metrics and the prediction probabilities\n",
    "    \"\"\"\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1] \n",
    "    performance = {\"Model\": name,\n",
    "                   \"Accuracy\": accuracy_score(y_test, y_pred),\n",
    "                   \"Precision\": precision_score(y_test, y_pred),\n",
    "                   \"Recall\": recall_score(y_test, y_pred),\n",
    "                   \"F1\": f1_score(y_test, y_pred)}\n",
    "    \n",
    "    return performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239ad70c-a9a9-4ae4-b686-23023404d285",
   "metadata": {},
   "source": [
    "#### K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "id": "7952542a-5f72-49b5-ba40-fffe5be22624",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_knn = trainAndTest(KNeighborsClassifier(n_neighbors=5),\n",
    "                               X_train_scaled, \n",
    "                               y_train, \n",
    "                               X_test_scaled, \n",
    "                               y_test, \n",
    "                              'KNN')\n",
    "baseline_performances.append(performance_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaeafb04-23d0-4752-b269-97eaf69caa16",
   "metadata": {},
   "source": [
    "#### Gaussian Naive Bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "id": "7ba50a3a-a0a9-4c56-a1ad-4157b4ffc308",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_gnb = trainAndTest(GaussianNB(),\n",
    "                               X_train_scaled, \n",
    "                               y_train, \n",
    "                               X_test_scaled, \n",
    "                               y_test,\n",
    "                              'GNB')\n",
    "baseline_performances.append(performance_gnb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f11a5d9-eb7a-481c-b8fd-f6be75eb63cd",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "id": "4bc629d0-45e8-4719-8ebb-e262d2ecfe31",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_lr = trainAndTest(LogisticRegression(max_iter=1000),\n",
    "                               X_train_scaled, \n",
    "                               y_train, \n",
    "                               X_test_scaled, \n",
    "                               y_test,\n",
    "                             'LR')\n",
    "baseline_performances.append(performance_lr)\n",
    "baseline_df = pd.DataFrame(baseline_performances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cd3205-b1ae-4d9d-92bf-12f3df930433",
   "metadata": {},
   "source": [
    "## Analysis: Refined Models\n",
    "\n",
    "### Dimensionality Reducation via PCA\n",
    "\n",
    "By transforming correlated features into a smaller set of uncorrelated components, PCA can help remove noise and redundant information, which may improve model performance. This is especially useful for algorithms like KNN that are sensitive to irrelevant or highly correlated features, and it can also make probabilistic models like Naive Bayes and Logistic Regression more robust. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "id": "36264f30-fe9c-4463-8784-12588a885d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=0.95, random_state=26) # using enough PCs to explain 95% of the variance\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239b5f3c-be89-4e11-97e2-90dbae4d49f9",
   "metadata": {},
   "source": [
    "### Parameter Tuning \n",
    "\n",
    "Choosing the right parameters can significantly improve model performance by balancing underfitting and overfitting.\n",
    "\n",
    "#### K-Nearest Neighbors\n",
    "\n",
    "The hyperparameters I will test for the KNN algorithm:\n",
    "- K: the number of neighbors considered per computation (3-21)\n",
    "- Weighting schema: uniform or weighted votes\n",
    "\n",
    "I'm keeping the distance metric as Euclidean and using F1-score to evaluate model fit/parameter performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "id": "c1c372d6-6cb0-4d44-b643-01853ffc2b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'metric': 'euclidean', 'n_neighbors': 5, 'weights': 'uniform'}\n"
     ]
    }
   ],
   "source": [
    "knn_param_grid = {\n",
    "    \"n_neighbors\": list(range(3, 22)),  \n",
    "    \"weights\": [\"uniform\", \"distance\"], \n",
    "    \"metric\": [\"euclidean\"]}\n",
    "knn_grid = GridSearchCV(KNeighborsClassifier(), \n",
    "                        knn_param_grid,\n",
    "                        cv=5,\n",
    "                        scoring=\"f1\")\n",
    "knn_grid.fit(X_train_pca, y_train)\n",
    "best_knn_params = knn_grid.best_params_\n",
    "print(\"Best parameters:\", best_knn_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12115ce-bd39-43fe-b3e0-a2976898dca0",
   "metadata": {},
   "source": [
    "Now, I can train/test the refined KNN model using the optimal parameters and PCA-transformed features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "id": "d30e3290-1ab5-4689-9a55-e91efdf9ce49",
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_performances = []\n",
    "performance_knn_refined = trainAndTest(KNeighborsClassifier(**best_knn_params),\n",
    "                                       X_train_pca, \n",
    "                                       y_train, \n",
    "                                       X_test_pca, \n",
    "                                       y_test, \n",
    "                                      'KNN')\n",
    "refined_performances.append(performance_knn_refined)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb30b56-801e-4474-9c3e-5c8aace1cdfc",
   "metadata": {},
   "source": [
    "#### Gaussian Naive Bayes \n",
    "\n",
    "There are no major hyperparameters to tune for the Gaussian NB algorithm, so I can go ahead and train the refined model using the pca-transformed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "id": "6dd61dcb-dbb8-4c31-a61c-bee93de318d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_gnb_refined = trainAndTest(GaussianNB(),\n",
    "                               X_train_pca, \n",
    "                               y_train, \n",
    "                               X_test_pca, \n",
    "                               y_test, \n",
    "                               'GNB')\n",
    "refined_performances.append(performance_gnb_refined)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208470f7-d152-4cae-a64f-afc4efcb1160",
   "metadata": {},
   "source": [
    "#### Logistic Regression\n",
    "\n",
    "The hyperparameter to be tuned for the Logistic Regression algorithm is C which defines the degree of regularization. I will test values on an expontial scale: 0.01, 0.1, 1, 10, 100. Again, I'm using F1-score to evaluate model fit/parameter performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "id": "af274df2-e53c-48f6-9d1d-3121df3ba210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Logistic Regression params: {'C': 1}\n"
     ]
    }
   ],
   "source": [
    "lr_grid = GridSearchCV(LogisticRegression(max_iter=1000), \n",
    "                       {\"C\": [0.01, 0.1, 1, 10, 100]},\n",
    "                       cv=5,                  \n",
    "                       scoring=\"f1\")\n",
    "lr_grid.fit(X_train_pca, y_train)\n",
    "best_lr_params = lr_grid.best_params_\n",
    "print(\"Best Logistic Regression params:\", best_lr_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33876c97-45e6-4cdd-aa41-8fde3fcc452f",
   "metadata": {},
   "source": [
    "The optimal value happens to be the same as the default value, so the refined LR model will be different from the baseline model in that it uses PCA-transformed training features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "id": "eae4c487-06b1-4fa7-8d3e-f84e5d178e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_lr_refined = trainAndTest(LogisticRegression(max_iter=1000, \n",
    "                                                        **best_lr_params),\n",
    "                               X_train_pca, \n",
    "                               y_train, \n",
    "                               X_test_pca, \n",
    "                               y_test, \n",
    "                               'LR')\n",
    "refined_performances.append(performance_lr_refined)\n",
    "refined_df = pd.DataFrame(refined_performances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040f2b78-5e66-4609-a787-2fce8ce8083d",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "id": "2709736d-4c97-4451-ba40-c5e454032417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Models Performances:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GNB</td>\n",
       "      <td>0.683333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.344828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LR</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.437500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Model  Accuracy  Precision    Recall        F1\n",
       "0   KNN  0.633333   0.200000  0.052632  0.083333\n",
       "1   GNB  0.683333   0.500000  0.263158  0.344828\n",
       "2    LR  0.700000   0.538462  0.368421  0.437500"
      ]
     },
     "execution_count": 578,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Baseline Models Performances:')\n",
    "baseline_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "id": "4162ad82-cd88-4575-b0e6-4f1987e2982e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refined Models Performances:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GNB</td>\n",
       "      <td>0.683333</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.387097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LR</td>\n",
       "      <td>0.716667</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.413793</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Model  Accuracy  Precision    Recall        F1\n",
       "0   KNN  0.633333        0.2  0.052632  0.083333\n",
       "1   GNB  0.683333        0.5  0.315789  0.387097\n",
       "2    LR  0.716667        0.6  0.315789  0.413793"
      ]
     },
     "execution_count": 579,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Refined Models Performances:')\n",
    "refined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c568cb-f683-4030-b68e-4064b33fb622",
   "metadata": {},
   "source": [
    "Looking at the baseline model performances, Logistic Regression (LR) achieves the highest accuracy (0.70) and F1-score (0.4375), followed by Gaussian Naive Bayes (GNB), with K-Nearest Neighbors (KNN) performing the worst. KNN struggles in this dataset, particularly in recall (0.0526), meaning it misses almost all patients who actually died. This poor performance may be due to the small dataset size and the fact that KNN is highly sensitive to feature scaling and irrelevant or correlated features. GNB and LR show more balanced results, with LR slightly outperforming GNB overall.\n",
    "\n",
    "After refinement with parameter tuning and PCA, the models show modest improvements. LR sees the largest gain in accuracy (0.7167) and precision (0.6), although its recall slightly decreases (0.3158), resulting in a small drop in F1 (0.4138). GNB improves in recall (0.3158) and F1-score (0.3871), indicating it catches more of the at-risk patients without sacrificing precision. KNN remains essentially unchanged, reinforcing that it is not well-suited to this dataset even after tuning and dimensionality reduction.\n",
    "\n",
    "Applying PCA reduces the dataset to fewer orthogonal features, which can affect the models differently. GNB may perform better because the independence assumption becomes more valid when features are uncorrelated. LR could lose some interpretability since the principal components are combinations of original features, but it may gain speed and stability. KNN might benefit if PCA reduces noise, though in this case the improvement appears limited.\n",
    "\n",
    "The differences in performance also reflect the underlying model assumptions. GNB assumes feature independence and normally distributed features, which may not fully hold in the raw dataset, limiting its predictive power. LR, by contrast, does not make these assumptions and can capture correlations between features, making it more reliable here.\n",
    "\n",
    "## Conclusion \n",
    "\n",
    "This analysis suggests that LR provides the most accurate and interpretable predictions for heart failure patient survival. GNB may still be useful for quickly identifying at-risk patients, especially after refinement and dimensionality reduction, but KNN appears unsuitable for this type of structured, correlated clinical data. These results highlight the importance of selecting models whose assumptions align with the characteristics of the dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
